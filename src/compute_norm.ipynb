{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import dataloader\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_node = socket.gethostname().split('.')[0]\n",
    "batcmd=f\"squeue -u billyli | grep {curr_node}\"\n",
    "curr_slurm = subprocess.check_output(batcmd, shell=True, text=True)\n",
    "slurm_id = curr_slurm.strip().split(' ')[0]\n",
    "local = f\"/local/slurm-{slurm_id}/local/audio/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/slurm-7021876/local/audio/data\n"
     ]
    }
   ],
   "source": [
    "print(local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set skip_norm as True only when you are computing the normalization stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datafiles/audioset_bal_unbal_train_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------the train dataloader---------------\n",
      "now using following mask: 12 freq, 56 time\n",
      "now using mix-up with rate 0.400000\n",
      "now process audioset_s\n",
      "now skip normalization (use it ONLY when you are computing the normalization stats).\n",
      "number of classes is 527\n"
     ]
    }
   ],
   "source": [
    "audio_conf = {'num_mel_bins': 64, 'target_length': 400, 'freqm': 12, 'timem': 56, 'mixup': 0.4, 'skip_norm': True, 'mode': 'train', 'dataset': 'audioset_s'}\n",
    "train_loader = torch.utils.data.DataLoader(dataloader.AudiosetDataset(os.path.join(local,'datafiles/audioset_bal_train_data.json'), label_csv=os.path.join(local, 'class_labels_indices.csv'),\n",
    "                                audio_conf=audio_conf), batch_size=1000, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqm 24 timem 75 mixup 0.5: -24.584522, 37.10214\n",
    "# freqm 12 timem 75 mixup 0.5: -27.248297 37.681698\n",
    "# freqm 12 timem 75 mixup 0.3: -29.072655 40.71728\n",
    "# freqm 12 timem 60 mixup 0.5: -27.712713 37.934616\n",
    "# freqm 12 timem 60 mixup 0.3: -29.686901 40.898224\n",
    "# freqm 12 timem 60 mixup 0.2: -30.733978 42.267876\n",
    "# freqm 12 timem 56 mixup 0.4: -28.799559 39.34941\n",
    "# freqm 12 timem 56 mixup 0.25: -30.333525 41.43794\n",
    "# freqm 10 timem 50 mixup 0.3: -30.68206 41.202892\n",
    "# freqm 10 timem 50 mixup 0.2: -31.451344 42.354965\n",
    "# freqm 6 timem 25 mixup 0.5: -30.446014 38.751587\n",
    "# freqm 6 timem 25 mixup 0.3: -32.60682 41.53026\n",
    "# freqm 6 timem 25 mixup 0.2: -33.632145 42.845398\n",
    "# freqm 6 timem 25 mixup 0.1: -34.673824 44.153343\n",
    "# freqm 0 timem 0 mixup 0: -38.193954 45.802143\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(-27.9030) tensor(38.8539)\n",
      "1 tensor(-28.1663) tensor(38.7850)\n",
      "2 tensor(-28.4667) tensor(38.8379)\n",
      "3 tensor(-28.9834) tensor(39.2770)\n",
      "4 tensor(-27.6454) tensor(38.4311)\n",
      "5 tensor(-29.0802) tensor(39.4005)\n",
      "6 tensor(-28.7869) tensor(38.9207)\n",
      "7 tensor(-28.2147) tensor(38.7805)\n",
      "8 tensor(-30.4817) tensor(41.1938)\n",
      "9 tensor(-29.0852) tensor(39.7420)\n",
      "10 tensor(-29.3313) tensor(40.0970)\n",
      "11 tensor(-30.1722) tensor(40.8762)\n",
      "12 tensor(-27.4719) tensor(39.1013)\n",
      "13 tensor(-28.7451) tensor(39.2319)\n",
      "14 tensor(-28.6019) tensor(39.8965)\n",
      "15 tensor(-29.2372) tensor(39.6468)\n",
      "16 tensor(-28.6448) tensor(38.3955)\n",
      "17 tensor(-29.4467) tensor(40.1634)\n",
      "18 tensor(-28.2285) tensor(37.9815)\n",
      "19 tensor(-28.9783) tensor(41.2987)\n",
      "20 tensor(-29.2136) tensor(39.9438)\n",
      "21 tensor(-28.7053) tensor(36.8321)\n",
      "-28.799559 39.34941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean=[]\n",
    "std=[]\n",
    "for i, (audio_input, labels) in enumerate(train_loader):\n",
    "    cur_mean = torch.mean(audio_input)\n",
    "    cur_std = torch.std(audio_input)\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "    print(i, cur_mean, cur_std)\n",
    "print(np.mean(mean), np.mean(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
