{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import dataloader\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_node = socket.gethostname().split('.')[0]\n",
    "batcmd=f\"squeue -u billyli | grep {curr_node}\"\n",
    "curr_slurm = subprocess.check_output(batcmd, shell=True, text=True)\n",
    "slurm_id = curr_slurm.strip().split(' ')[0]\n",
    "local = f\"/local/slurm-{slurm_id}/local/audio/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local/slurm-7002045/local/audio/data\n"
     ]
    }
   ],
   "source": [
    "print(local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set skip_norm as True only when you are computing the normalization stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "datafiles/audioset_bal_unbal_train_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------the train dataloader---------------\n",
      "now using following mask: 24 freq, 75 time\n",
      "now using mix-up with rate 0.500000\n",
      "now process audioset_s\n",
      "now skip normalization (use it ONLY when you are computing the normalization stats).\n",
      "number of classes is 527\n"
     ]
    }
   ],
   "source": [
    "audio_conf = {'num_mel_bins': 64, 'target_length': 400, 'freqm': 24, 'timem': 75, 'mixup': 0.5, 'skip_norm': True, 'mode': 'train', 'dataset': 'audioset_s'}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudiosetDataset(os.path.join(local,'datafiles/audioset_bal_train_data.json'), label_csv=os.path.join(local, 'class_labels_indices.csv'),\n",
    "                                audio_conf=audio_conf), batch_size=1000, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(-23.6115) tensor(36.6209)\n",
      "1 tensor(-24.7359) tensor(37.1037)\n",
      "2 tensor(-23.9272) tensor(36.3471)\n",
      "3 tensor(-24.4302) tensor(36.3832)\n",
      "4 tensor(-24.1501) tensor(36.8541)\n",
      "5 tensor(-24.1723) tensor(36.8703)\n",
      "6 tensor(-24.0542) tensor(36.2084)\n",
      "7 tensor(-24.2079) tensor(36.7262)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mean=[]\n",
    "std=[]\n",
    "for i, (audio_input, labels) in enumerate(train_loader):\n",
    "    cur_mean = torch.mean(audio_input)\n",
    "    cur_std = torch.std(audio_input)\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "    print(i, cur_mean, cur_std)\n",
    "print(np.mean(mean), np.mean(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
